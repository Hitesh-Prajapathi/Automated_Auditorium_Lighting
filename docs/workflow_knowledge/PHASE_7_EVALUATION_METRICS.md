# PHASE 7: EVALUATION METRICS

## 1ï¸âƒ£ PHASE PURPOSE

### What This Phase Exists For
Phase 7 provides **research-grade metrics** to evaluate the quality and consistency of lighting decisions. It is purely observational.

### What Problem It Solves
How do we know if the lighting system is working well?
- Are decisions consistent across runs?
- Is the LLM producing stable outputs?
- Are we using the full range of lighting options?

Phase 7 answers these questions with quantifiable metrics.

### What Question It Answers
> "How good is our lighting system?"

---

## 2ï¸âƒ£ DIRECTORY & FILE BREAKDOWN

### Directory: `phase_7/`

```
phase_7/
â”œâ”€â”€ __init__.py               # Module exports
â”œâ”€â”€ README.md                 # Phase documentation
â”œâ”€â”€ demo.py                   # Demo script
â”œâ”€â”€ metrics.py                # Unified metrics engine
â”œâ”€â”€ schemas.py                # Metric output schemas
â”œâ”€â”€ trace_logger.py           # Execution trace logging
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ consistency.py        # Jaccard, determinism, drift
â”‚   â”œâ”€â”€ coverage.py           # Group coverage, diversity
â”‚   â””â”€â”€ stability.py          # Cross-run stability
â””â”€â”€ experiment_configs/
    â”œâ”€â”€ ablation.yaml         # Ablation study config
    â””â”€â”€ baseline.yaml         # Baseline experiment config
```

---

### File: `phase_7/__init__.py`

**Exports:**
- `MetricsEngine` (from `metrics.py`)
- Evaluation functions

---

### File: `phase_7/metrics.py`

**Why This File Exists:**
Unified interface to all metrics. Contains `MetricsEngine` class.

---

### File: `phase_7/trace_logger.py`

**Why This File Exists:**
Logs execution traces for debugging and analysis.

---

### File: `phase_7/evaluation/consistency.py`

**Why This File Exists:**
Measures how consistent lighting decisions are.

**Key Functions:**
- `compute_jaccard_similarity(set_a, set_b)` â€” Group overlap
- `compute_determinism_score(instr_a, instr_b)` â€” Structural similarity
- `compute_drift_score(instructions)` â€” Sequence stability

---

### File: `phase_7/evaluation/coverage.py`

**Why This File Exists:**
Detects trivial solutions (using only one group, constant intensity).

**Key Functions:**
- `compute_group_coverage(instruction, available_groups)` â€” Utilization ratio
- `compute_parameter_diversity(instruction)` â€” Parameter variety

---

### File: `phase_7/evaluation/stability.py`

**Why This File Exists:**
Measures cross-run consistency for LLM outputs.

**Key Functions:**
- `compute_cross_run_stability(runs)` â€” How stable are outputs across runs?
- `compute_pairwise_stability(runs)` â€” Pairwise comparison

---

## 3ï¸âƒ£ CLASS-BY-CLASS ANALYSIS

### Class: `MetricsEngine`

**Purpose:**
Unified interface for computing all metrics.

**Constructor:**
```python
def __init__(self, available_groups: Optional[Set[str]] = None):
```

**Internal State:**
- `self.available_groups`: Set of known group IDs for coverage calculation

**Key Methods:**
- `evaluate_instruction(instruction)` â†’ coverage + diversity
- `evaluate_pair(instr_a, instr_b)` â†’ determinism score
- `evaluate_sequence(instructions)` â†’ drift score
- `evaluate_runs(runs)` â†’ cross-run stability
- `generate_report(instructions, runs)` â†’ comprehensive report

---

## 4ï¸âƒ£ FUNCTION-BY-FUNCTION ANALYSIS

### `consistency.compute_jaccard_similarity(set_a, set_b) -> float`

**Exact Responsibility:**
Compute Jaccard similarity: |A âˆ© B| / |A âˆª B|

**Returns:** 0.0 (no overlap) to 1.0 (identical)

---

### `consistency.compute_determinism_score(instr_a, instr_b, epsilon) -> Tuple[float, dict]`

**Exact Responsibility:**
Measure structural similarity between two instructions.

**Definition of "Determinism":**
- Same group_ids selected
- Same transition types
- Intensity within Îµ = Â±0.05

**Returns:** 
- Score (0-1)
- Breakdown dict with sub-scores

---

### `consistency.compute_drift_score(instructions) -> float`

**Exact Responsibility:**
Measure how much lighting changes between consecutive scenes.

**Returns:** Average drift (0 = stable, 1 = completely different each time)

---

### `coverage.compute_group_coverage(instruction, available_groups) -> float`

**Exact Responsibility:**
What fraction of available groups are used?

**Returns:** 0.0 (none) to 1.0 (all)

---

### `coverage.compute_parameter_diversity(instruction) -> dict`

**Exact Responsibility:**
Measure variety in lighting parameters.

**Returns:**
```python
{
    "intensity_range": float,      # max - min intensity
    "transition_types": int,       # unique transition types
    "colors_used": int,            # unique colors
    "groups_used": int             # total groups
}
```

---

### `stability.compute_cross_run_stability(runs, epsilon) -> dict`

**Exact Responsibility:**
How consistent are results across multiple pipeline runs?

**Input:** List of runs, each run is a list of LightingInstructions

**Returns:**
```python
{
    "stability_score": float,  # 0-1
    "num_runs": int,
    "epsilon": float
}
```

---

### `MetricsEngine.generate_report(instructions, runs) -> dict`

**Exact Responsibility:**
Generate comprehensive metrics report.

**Output:**
```python
{
    "summary": {
        "num_instructions": int,
        "available_groups": int
    },
    "sequence_metrics": {
        "drift_score": float,
        "num_instructions": int
    },
    "instruction_metrics": [
        {
            "index": int,
            "scene_id": str,
            "coverage": float,
            "diversity": dict
        }
    ],
    "stability_metrics": dict  # if runs provided
}
```

---

## 5ï¸âƒ£ EXECUTION FLOW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 7 EXECUTION FLOW                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. INPUT: List of LightingInstruction from Phase 4
          â”‚
          â–¼
2. MetricsEngine instantiation
   â””â”€â”€ Configure available_groups
          â”‚
          â–¼
3. Evaluate each instruction:
   â”œâ”€â”€ compute_group_coverage()
   â””â”€â”€ compute_parameter_diversity()
          â”‚
          â–¼
4. Evaluate sequence:
   â””â”€â”€ compute_drift_score()
          â”‚
          â–¼
5. If multiple runs provided:
   â””â”€â”€ compute_cross_run_stability()
          â”‚
          â–¼
6. OUTPUT: Comprehensive metrics report
```

---

## 6ï¸âƒ£ DATA STRUCTURES & MODELS

### Input: LightingInstruction
Same structure as Phase 4 output.

### Output: Metrics Report
```python
{
    "summary": {...},
    "sequence_metrics": {...},
    "instruction_metrics": [...],
    "stability_metrics": {...}  # optional
}
```

---

## 7ï¸âƒ£ FAILURE BEHAVIOR

### Empty Instruction List
- Returns empty report
- **Continues**

### Single Instruction (No Sequence)
- Drift score = 0.0
- **Continues**

### Single Run (No Stability)
- Stability score = 1.0
- **Continues**

**Phase 7 cannot fail the pipeline.**

---

## 8ï¸âƒ£ PHASE BOUNDARIES

### What Phase 7 MUST NOT Do:
- âŒ Must NOT modify LightingInstructions
- âŒ Must NOT influence execution
- âŒ Must NOT control lighting (that's Phase 8)
- âŒ Must NOT make decisions (that's Phase 4)

### What Phase 7 MUST Do:
- âœ… Observe outputs without modification
- âœ… Compute reproducible metrics
- âœ… Provide actionable insights

### Principle: OBSERVATION â‰  CONTROL
Phase 7 measures. It doesn't change anything.

---

## 9ï¸âƒ£ COMMON CONFUSION CLARIFICATION

### "Why is determinism defined structurally, not bytewise?"

LLM outputs vary slightly in formatting.
Structural determinism compares:
- Same groups selected
- Intensity within tolerance
- Same transition types

This is fair, reproducible, and defensible in research papers.

### "Why is epsilon = 0.05 for intensity?"

5% tolerance accounts for:
- LLM randomness
- Floating point precision
- Negligible perceptual difference

### "Why does drift use 1 - similarity instead of raw difference?"

Drift should be high when things change a lot.
1 - similarity gives: 0 = stable, 1 = maximum change.

### "Why are there experiment configs in YAML?"

For research: ablation studies, baseline comparisons.
YAML is human-readable and version-controllable.

---

## ğŸ”Ÿ SUMMARY & GUARANTEES

### Guarantees Provided by Phase 7:
1. **Non-invasive**: Never modifies input
2. **Reproducible**: Same inputs â†’ same metrics
3. **Comprehensive**: Coverage, consistency, stability

### Invariants Maintained:
- All scores are in [0, 1]
- Reports are JSON-serializable
- No side effects

### What Would Break If Phase 7 Were Removed:
- No quality metrics
- No way to evaluate system performance
- No research validation
- Debugging becomes harder
